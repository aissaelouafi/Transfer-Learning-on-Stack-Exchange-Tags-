16/10/29 20:05:45 INFO SparkContext: Running Spark version 1.6.2
16/10/29 20:05:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/29 20:05:45 INFO SecurityManager: Changing view acls to: aissaelouafi
16/10/29 20:05:45 INFO SecurityManager: Changing modify acls to: aissaelouafi
16/10/29 20:05:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(aissaelouafi); users with modify permissions: Set(aissaelouafi)
16/10/29 20:05:46 INFO Utils: Successfully started service 'sparkDriver' on port 58853.
16/10/29 20:05:46 INFO Slf4jLogger: Slf4jLogger started
16/10/29 20:05:46 INFO Remoting: Starting remoting
16/10/29 20:05:47 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:58855]
16/10/29 20:05:47 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 58855.
16/10/29 20:05:47 INFO SparkEnv: Registering MapOutputTracker
16/10/29 20:05:47 INFO SparkEnv: Registering BlockManagerMaster
16/10/29 20:05:47 INFO DiskBlockManager: Created local directory at /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/blockmgr-d56957b8-8b51-4344-b170-b529e55cd20f
16/10/29 20:05:47 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
16/10/29 20:05:47 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/29 20:05:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/29 20:05:47 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
16/10/29 20:05:47 INFO HttpFileServer: HTTP File server directory is /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/httpd-649ba192-361a-44be-bc58-58ba74685433
16/10/29 20:05:47 INFO HttpServer: Starting HTTP Server
16/10/29 20:05:47 INFO Utils: Successfully started service 'HTTP file server' on port 58857.
16/10/29 20:05:47 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/com.databricks_spark-csv_2.11-1.3.0.jar at http://127.0.0.1:58857/jars/com.databricks_spark-csv_2.11-1.3.0.jar with timestamp 1477764347715
16/10/29 20:05:47 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar at http://127.0.0.1:58857/jars/org.apache.commons_commons-csv-1.1.jar with timestamp 1477764347717
16/10/29 20:05:47 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar at http://127.0.0.1:58857/jars/com.univocity_univocity-parsers-1.5.1.jar with timestamp 1477764347719
16/10/29 20:05:47 INFO SparkContext: Added JAR file:/usr/local/lib/R/3.2/site-library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:58857/jars/sparklyr-1.6-2.10.jar with timestamp 1477764347721
16/10/29 20:05:47 INFO Executor: Starting executor ID driver on host localhost
16/10/29 20:05:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58858.
16/10/29 20:05:47 INFO NettyBlockTransferService: Server created on 58858
16/10/29 20:05:47 INFO BlockManagerMaster: Trying to register BlockManager
16/10/29 20:05:47 INFO BlockManagerMasterEndpoint: Registering block manager localhost:58858 with 511.1 MB RAM, BlockManagerId(driver, localhost, 58858)
16/10/29 20:05:47 INFO BlockManagerMaster: Registered BlockManager
16/10/29 20:05:49 INFO HiveContext: Initializing execution hive, version 1.2.1
16/10/29 20:05:49 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
16/10/29 20:05:49 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/10/29 20:05:49 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/10/29 20:05:49 INFO ObjectStore: ObjectStore, initialize called
16/10/29 20:05:50 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/10/29 20:05:50 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/10/29 20:05:50 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/10/29 20:05:50 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/10/29 20:05:52 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/10/29 20:05:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:53 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:53 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/10/29 20:05:53 INFO ObjectStore: Initialized ObjectStore
16/10/29 20:05:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/10/29 20:05:54 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/10/29 20:05:54 INFO HiveMetaStore: Added admin role in metastore
16/10/29 20:05:54 INFO HiveMetaStore: Added public role in metastore
16/10/29 20:05:54 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/10/29 20:05:54 INFO HiveMetaStore: 0: get_all_databases
16/10/29 20:05:54 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_all_databases	
16/10/29 20:05:54 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/10/29 20:05:54 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/10/29 20:05:54 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:54 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi
16/10/29 20:05:54 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/aissaelouafi
16/10/29 20:05:54 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/cf4bbc57-03ef-4a88-9652-b1e7faf55fb7_resources
16/10/29 20:05:54 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/cf4bbc57-03ef-4a88-9652-b1e7faf55fb7
16/10/29 20:05:54 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/aissaelouafi/cf4bbc57-03ef-4a88-9652-b1e7faf55fb7
16/10/29 20:05:54 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/cf4bbc57-03ef-4a88-9652-b1e7faf55fb7/_tmp_space.db
16/10/29 20:05:54 INFO HiveContext: default warehouse location is /user/hive/warehouse
16/10/29 20:05:54 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/10/29 20:05:54 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
16/10/29 20:05:54 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/10/29 20:05:55 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/10/29 20:05:55 INFO ObjectStore: ObjectStore, initialize called
16/10/29 20:05:55 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/10/29 20:05:55 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/10/29 20:05:55 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/10/29 20:05:55 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/10/29 20:05:56 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/10/29 20:05:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/10/29 20:05:58 INFO ObjectStore: Initialized ObjectStore
16/10/29 20:05:58 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/10/29 20:05:58 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/10/29 20:05:58 INFO HiveMetaStore: Added admin role in metastore
16/10/29 20:05:58 INFO HiveMetaStore: Added public role in metastore
16/10/29 20:05:58 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/10/29 20:05:59 INFO HiveMetaStore: 0: get_all_databases
16/10/29 20:05:59 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_all_databases	
16/10/29 20:05:59 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/10/29 20:05:59 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/10/29 20:05:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/10/29 20:05:59 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/9caac448-0048-4299-8ffb-e16e5c32d8f4_resources
16/10/29 20:05:59 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/9caac448-0048-4299-8ffb-e16e5c32d8f4
16/10/29 20:05:59 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/aissaelouafi/9caac448-0048-4299-8ffb-e16e5c32d8f4
16/10/29 20:05:59 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/9caac448-0048-4299-8ffb-e16e5c32d8f4/_tmp_space.db
16/10/29 20:06:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:06:00 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:09:08 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:09:08 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:09:08 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:09:08 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:09:08 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
16/10/29 20:09:08 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:09:08 INFO DAGScheduler: Missing parents: List()
16/10/29 20:09:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:56), which has no missing parents
16/10/29 20:09:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
16/10/29 20:09:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
16/10/29 20:09:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:09:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/10/29 20:09:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:56)
16/10/29 20:09:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/29 20:09:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2396 bytes)
16/10/29 20:09:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/29 20:09:09 INFO Executor: Fetching http://127.0.0.1:58857/jars/org.apache.commons_commons-csv-1.1.jar with timestamp 1477764347717
16/10/29 20:09:09 INFO Utils: Fetching http://127.0.0.1:58857/jars/org.apache.commons_commons-csv-1.1.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/fetchFileTemp7172354901647678588.tmp
16/10/29 20:09:09 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/org.apache.commons_commons-csv-1.1.jar to class loader
16/10/29 20:09:09 INFO Executor: Fetching http://127.0.0.1:58857/jars/com.univocity_univocity-parsers-1.5.1.jar with timestamp 1477764347719
16/10/29 20:09:09 INFO Utils: Fetching http://127.0.0.1:58857/jars/com.univocity_univocity-parsers-1.5.1.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/fetchFileTemp8918027734786272294.tmp
16/10/29 20:09:09 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/com.univocity_univocity-parsers-1.5.1.jar to class loader
16/10/29 20:09:09 INFO Executor: Fetching http://127.0.0.1:58857/jars/sparklyr-1.6-2.10.jar with timestamp 1477764347721
16/10/29 20:09:09 INFO Utils: Fetching http://127.0.0.1:58857/jars/sparklyr-1.6-2.10.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/fetchFileTemp5653850166229728386.tmp
16/10/29 20:09:09 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/sparklyr-1.6-2.10.jar to class loader
16/10/29 20:09:09 INFO Executor: Fetching http://127.0.0.1:58857/jars/com.databricks_spark-csv_2.11-1.3.0.jar with timestamp 1477764347715
16/10/29 20:09:09 INFO Utils: Fetching http://127.0.0.1:58857/jars/com.databricks_spark-csv_2.11-1.3.0.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/fetchFileTemp3153018781556463563.tmp
16/10/29 20:09:09 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-0a9f112a-45f8-443f-b690-ee39937e8d19/userFiles-aab88b1c-728a-4b9c-b303-d59876446ba6/com.databricks_spark-csv_2.11-1.3.0.jar to class loader
16/10/29 20:09:09 INFO GenerateUnsafeProjection: Code generated in 214.296678 ms
16/10/29 20:09:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
16/10/29 20:09:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 466 ms on localhost (1/1)
16/10/29 20:09:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/29 20:09:09 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0,488 s
16/10/29 20:09:09 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0,670796 s
16/10/29 20:09:09 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:09:09 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:09:48 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:09:48 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:09:48 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:09:48 INFO DAGScheduler: Got job 1 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:09:48 INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:59)
16/10/29 20:09:48 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:09:48 INFO DAGScheduler: Missing parents: List()
16/10/29 20:09:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at map at utils.scala:56), which has no missing parents
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.4 KB, free 13.8 KB)
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KB, free 16.8 KB)
16/10/29 20:09:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:09:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
16/10/29 20:09:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at map at utils.scala:56)
16/10/29 20:09:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/10/29 20:09:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2716 bytes)
16/10/29 20:09:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
16/10/29 20:09:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1089 bytes result sent to driver
16/10/29 20:09:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 13 ms on localhost (1/1)
16/10/29 20:09:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/10/29 20:09:48 INFO DAGScheduler: ResultStage 1 (collect at utils.scala:59) finished in 0,013 s
16/10/29 20:09:48 INFO DAGScheduler: Job 1 finished: collect at utils.scala:59, took 0,031013 s
16/10/29 20:09:48 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:09:48 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 61.8 KB, free 78.6 KB)
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.3 KB, free 97.9 KB)
16/10/29 20:09:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:09:48 INFO SparkContext: Created broadcast 2 from textFile at TextFile.scala:30
16/10/29 20:09:48 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:09:48 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/10/29 20:09:48 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
16/10/29 20:09:48 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
16/10/29 20:09:48 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:09:48 INFO DAGScheduler: Missing parents: List()
16/10/29 20:09:48 INFO DAGScheduler: Submitting ResultStage 2 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a12646f082c.csv MapPartitionsRDD[12] at textFile at TextFile.scala:30), which has no missing parents
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 101.1 KB)
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1900.0 B, free 102.9 KB)
16/10/29 20:09:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:58858 (size: 1900.0 B, free: 511.1 MB)
16/10/29 20:09:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
16/10/29 20:09:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a12646f082c.csv MapPartitionsRDD[12] at textFile at TextFile.scala:30)
16/10/29 20:09:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
16/10/29 20:09:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:09:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
16/10/29 20:09:48 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12646f082c.csv:0+4925434
16/10/29 20:09:48 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/10/29 20:09:48 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/10/29 20:09:48 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/10/29 20:09:48 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/10/29 20:09:48 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/10/29 20:09:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3021 bytes result sent to driver
16/10/29 20:09:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 55 ms on localhost (1/1)
16/10/29 20:09:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/10/29 20:09:48 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0,057 s
16/10/29 20:09:48 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0,075928 s
16/10/29 20:09:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 208.5 KB, free 311.4 KB)
16/10/29 20:09:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.3 KB, free 330.8 KB)
16/10/29 20:09:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:09:49 INFO SparkContext: Created broadcast 4 from textFile at TextFile.scala:30
16/10/29 20:09:49 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:09:49 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
16/10/29 20:09:49 INFO DAGScheduler: Registering RDD 22 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:09:49 INFO DAGScheduler: Got job 3 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/10/29 20:09:49 INFO DAGScheduler: Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:09:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
16/10/29 20:09:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
16/10/29 20:09:49 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/10/29 20:09:49 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 17.3 KB, free 348.0 KB)
16/10/29 20:09:49 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.3 KB, free 356.4 KB)
16/10/29 20:09:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:58858 (size: 8.3 KB, free: 511.1 MB)
16/10/29 20:09:49 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/10/29 20:09:49 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:09:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
16/10/29 20:09:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:09:49 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:09:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
16/10/29 20:09:49 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
16/10/29 20:09:49 INFO CacheManager: Partition rdd_19_1 not found, computing it
16/10/29 20:09:49 INFO CacheManager: Partition rdd_19_0 not found, computing it
16/10/29 20:09:49 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12646f082c.csv:4925434+4925435
16/10/29 20:09:49 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12646f082c.csv:0+4925434
16/10/29 20:09:49 INFO GenerateUnsafeProjection: Code generated in 18.832289 ms
16/10/29 20:09:49 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:09:49 ERROR CsvRelation$: Exception while parsing line: 1,"What is the criticality of the ribosome binding site relative to the start codon in prokaryotic translation?","<p>In prokaryotic translation, how critical for efficient translation is the location of the ribosome binding site, relative to the start codon?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:09:49 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:09:49 ERROR Executor: Exception in task 1.0 in stage 3.0 (TID 4)
java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:09:49 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:09:49 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:09:49 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
16/10/29 20:09:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/10/29 20:09:49 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 4, localhost): java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:09:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/10/29 20:09:49 INFO TaskSchedulerImpl: Cancelling stage 3
16/10/29 20:09:49 INFO DAGScheduler: ShuffleMapStage 3 (sql at NativeMethodAccessorImpl.java:-2) failed in 0,139 s
16/10/29 20:09:49 INFO DAGScheduler: Job 3 failed: sql at NativeMethodAccessorImpl.java:-2, took 0,197398 s
16/10/29 20:10:01 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:10:01 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:10:01 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:10:01 INFO DAGScheduler: Got job 4 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:10:01 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:59)
16/10/29 20:10:01 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:10:01 INFO DAGScheduler: Missing parents: List()
16/10/29 20:10:01 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at map at utils.scala:56), which has no missing parents
16/10/29 20:10:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.4 KB, free 361.8 KB)
16/10/29 20:10:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 364.8 KB)
16/10/29 20:10:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:10:01 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
16/10/29 20:10:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at map at utils.scala:56)
16/10/29 20:10:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
16/10/29 20:10:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2757 bytes)
16/10/29 20:10:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
16/10/29 20:10:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1099 bytes result sent to driver
16/10/29 20:10:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 11 ms on localhost (1/1)
16/10/29 20:10:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
16/10/29 20:10:01 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:59) finished in 0,012 s
16/10/29 20:10:01 INFO DAGScheduler: Job 4 finished: collect at utils.scala:59, took 0,025759 s
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 11
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 10
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 9
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 8
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:58858 in memory (size: 1900.0 B, free: 511.1 MB)
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 6
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 5
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 4
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 3
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 2
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 18
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 17
16/10/29 20:10:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:58858 in memory (size: 8.3 KB, free: 511.1 MB)
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 16
16/10/29 20:10:47 INFO ContextCleaner: Cleaned shuffle 0
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 15
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 14
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 13
16/10/29 20:10:47 INFO ContextCleaner: Cleaned accumulator 12
16/10/29 20:12:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:36 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:36 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:12:36 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:12:36 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:59)
16/10/29 20:12:36 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:36 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[33] at map at utils.scala:56), which has no missing parents
16/10/29 20:12:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.4 KB, free 233.3 KB)
16/10/29 20:12:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KB, free 236.2 KB)
16/10/29 20:12:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:12:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at map at utils.scala:56)
16/10/29 20:12:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
16/10/29 20:12:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2757 bytes)
16/10/29 20:12:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
16/10/29 20:12:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1099 bytes result sent to driver
16/10/29 20:12:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 11 ms on localhost (1/1)
16/10/29 20:12:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
16/10/29 20:12:36 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:59) finished in 0,014 s
16/10/29 20:12:36 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0,028802 s
16/10/29 20:12:36 INFO MapPartitionsRDD: Removing RDD 19 from persistence list
16/10/29 20:12:36 INFO BlockManager: Removing RDD 19
16/10/29 20:12:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:36 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:36 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:12:36 INFO DAGScheduler: Got job 6 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:12:36 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:59)
16/10/29 20:12:36 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:36 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:36 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[37] at map at utils.scala:56), which has no missing parents
16/10/29 20:12:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 5.4 KB, free 241.7 KB)
16/10/29 20:12:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.0 KB, free 244.6 KB)
16/10/29 20:12:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:12:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[37] at map at utils.scala:56)
16/10/29 20:12:36 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
16/10/29 20:12:36 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 2716 bytes)
16/10/29 20:12:36 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
16/10/29 20:12:36 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1089 bytes result sent to driver
16/10/29 20:12:36 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (1/1)
16/10/29 20:12:36 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:59) finished in 0,006 s
16/10/29 20:12:36 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
16/10/29 20:12:36 INFO DAGScheduler: Job 6 finished: collect at utils.scala:59, took 0,017833 s
16/10/29 20:12:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:36 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 208.5 KB, free 453.2 KB)
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.3 KB, free 472.5 KB)
16/10/29 20:12:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:12:37 INFO SparkContext: Created broadcast 9 from textFile at TextFile.scala:30
16/10/29 20:12:37 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:12:37 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/10/29 20:12:37 INFO DAGScheduler: Got job 7 (take at CsvRelation.scala:249) with 1 output partitions
16/10/29 20:12:37 INFO DAGScheduler: Final stage: ResultStage 8 (take at CsvRelation.scala:249)
16/10/29 20:12:37 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:37 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:37 INFO DAGScheduler: Submitting ResultStage 8 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a123581b437.csv MapPartitionsRDD[40] at textFile at TextFile.scala:30), which has no missing parents
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 475.6 KB)
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1902.0 B, free 477.5 KB)
16/10/29 20:12:37 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:58858 (size: 1902.0 B, free: 511.1 MB)
16/10/29 20:12:37 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a123581b437.csv MapPartitionsRDD[40] at textFile at TextFile.scala:30)
16/10/29 20:12:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
16/10/29 20:12:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:12:37 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
16/10/29 20:12:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a123581b437.csv:0+4925434
16/10/29 20:12:37 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3021 bytes result sent to driver
16/10/29 20:12:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 14 ms on localhost (1/1)
16/10/29 20:12:37 INFO DAGScheduler: ResultStage 8 (take at CsvRelation.scala:249) finished in 0,014 s
16/10/29 20:12:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
16/10/29 20:12:37 INFO DAGScheduler: Job 7 finished: take at CsvRelation.scala:249, took 0,034410 s
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 208.5 KB, free 686.0 KB)
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 19.3 KB, free 705.3 KB)
16/10/29 20:12:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:12:37 INFO SparkContext: Created broadcast 11 from textFile at TextFile.scala:30
16/10/29 20:12:37 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:12:37 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
16/10/29 20:12:37 INFO DAGScheduler: Registering RDD 50 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:37 INFO DAGScheduler: Got job 8 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/10/29 20:12:37 INFO DAGScheduler: Final stage: ResultStage 10 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
16/10/29 20:12:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
16/10/29 20:12:37 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[50] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 17.3 KB, free 722.6 KB)
16/10/29 20:12:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.3 KB, free 730.9 KB)
16/10/29 20:12:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:58858 (size: 8.3 KB, free: 511.1 MB)
16/10/29 20:12:37 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:37 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[50] at sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:37 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks
16/10/29 20:12:37 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:12:37 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 10, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:12:37 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
16/10/29 20:12:37 INFO Executor: Running task 1.0 in stage 9.0 (TID 10)
16/10/29 20:12:37 INFO CacheManager: Partition rdd_47_0 not found, computing it
16/10/29 20:12:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a123581b437.csv:0+4925434
16/10/29 20:12:37 INFO CacheManager: Partition rdd_47_1 not found, computing it
16/10/29 20:12:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a123581b437.csv:4925434+4925435
16/10/29 20:12:37 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:12:37 ERROR CsvRelation$: Exception while parsing line: 1,"What is the criticality of the ribosome binding site relative to the start codon in prokaryotic translation?","<p>In prokaryotic translation, how critical for efficient translation is the location of the ribosome binding site, relative to the start codon?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:37 ERROR Executor: Exception in task 1.0 in stage 9.0 (TID 10)
java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:37 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:12:37 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)
java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:37 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 9, localhost): java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:12:37 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job
16/10/29 20:12:37 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
16/10/29 20:12:37 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 10, localhost): java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:12:37 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
16/10/29 20:12:37 INFO TaskSchedulerImpl: Cancelling stage 9
16/10/29 20:12:37 INFO DAGScheduler: ShuffleMapStage 9 (sql at NativeMethodAccessorImpl.java:-2) failed in 0,049 s
16/10/29 20:12:37 INFO DAGScheduler: Job 8 failed: sql at NativeMethodAccessorImpl.java:-2, took 0,072812 s
16/10/29 20:12:52 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:52 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:52 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:12:52 INFO DAGScheduler: Got job 9 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:12:52 INFO DAGScheduler: Final stage: ResultStage 11 (collect at utils.scala:59)
16/10/29 20:12:52 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:52 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:52 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[57] at map at utils.scala:56), which has no missing parents
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.4 KB, free 736.3 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.0 KB, free 739.3 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[57] at map at utils.scala:56)
16/10/29 20:12:52 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
16/10/29 20:12:52 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2757 bytes)
16/10/29 20:12:52 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
16/10/29 20:12:52 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1099 bytes result sent to driver
16/10/29 20:12:52 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 8 ms on localhost (1/1)
16/10/29 20:12:52 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
16/10/29 20:12:52 INFO DAGScheduler: ResultStage 11 (collect at utils.scala:59) finished in 0,008 s
16/10/29 20:12:52 INFO DAGScheduler: Job 9 finished: collect at utils.scala:59, took 0,022766 s
16/10/29 20:12:52 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:52 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:52 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:12:52 INFO DAGScheduler: Got job 10 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:12:52 INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:59)
16/10/29 20:12:52 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:52 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:52 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[61] at map at utils.scala:56), which has no missing parents
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.4 KB, free 744.7 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.0 KB, free 747.7 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[61] at map at utils.scala:56)
16/10/29 20:12:52 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
16/10/29 20:12:52 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, partition 0,PROCESS_LOCAL, 2757 bytes)
16/10/29 20:12:52 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
16/10/29 20:12:52 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1099 bytes result sent to driver
16/10/29 20:12:52 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 5 ms on localhost (1/1)
16/10/29 20:12:52 INFO DAGScheduler: ResultStage 12 (collect at utils.scala:59) finished in 0,006 s
16/10/29 20:12:52 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
16/10/29 20:12:52 INFO DAGScheduler: Job 10 finished: collect at utils.scala:59, took 0,019683 s
16/10/29 20:12:52 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:12:52 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 208.5 KB, free 956.2 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.3 KB, free 975.6 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 15 from textFile at TextFile.scala:30
16/10/29 20:12:52 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:12:52 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/10/29 20:12:52 INFO DAGScheduler: Got job 11 (take at CsvRelation.scala:249) with 1 output partitions
16/10/29 20:12:52 INFO DAGScheduler: Final stage: ResultStage 13 (take at CsvRelation.scala:249)
16/10/29 20:12:52 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:12:52 INFO DAGScheduler: Missing parents: List()
16/10/29 20:12:52 INFO DAGScheduler: Submitting ResultStage 13 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a12565ea252.csv MapPartitionsRDD[64] at textFile at TextFile.scala:30), which has no missing parents
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 3.1 KB, free 978.7 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1903.0 B, free 980.6 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:58858 (size: 1903.0 B, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a12565ea252.csv MapPartitionsRDD[64] at textFile at TextFile.scala:30)
16/10/29 20:12:52 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
16/10/29 20:12:52 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:12:52 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
16/10/29 20:12:52 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12565ea252.csv:0+6784485
16/10/29 20:12:52 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 3212 bytes result sent to driver
16/10/29 20:12:52 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 8 ms on localhost (1/1)
16/10/29 20:12:52 INFO DAGScheduler: ResultStage 13 (take at CsvRelation.scala:249) finished in 0,008 s
16/10/29 20:12:52 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
16/10/29 20:12:52 INFO DAGScheduler: Job 11 finished: take at CsvRelation.scala:249, took 0,020432 s
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 208.5 KB, free 1189.1 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 19.3 KB, free 1208.4 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 17 from textFile at TextFile.scala:30
16/10/29 20:12:52 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:12:52 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
16/10/29 20:12:52 INFO DAGScheduler: Registering RDD 74 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:52 INFO DAGScheduler: Got job 12 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/10/29 20:12:52 INFO DAGScheduler: Final stage: ResultStage 15 (sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
16/10/29 20:12:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 14)
16/10/29 20:12:52 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[74] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 17.3 KB, free 1225.6 KB)
16/10/29 20:12:52 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1234.0 KB)
16/10/29 20:12:52 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:58858 (size: 8.3 KB, free: 511.0 MB)
16/10/29 20:12:52 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
16/10/29 20:12:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[74] at sql at NativeMethodAccessorImpl.java:-2)
16/10/29 20:12:52 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
16/10/29 20:12:52 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:12:52 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:12:52 INFO Executor: Running task 1.0 in stage 14.0 (TID 15)
16/10/29 20:12:52 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
16/10/29 20:12:52 INFO CacheManager: Partition rdd_71_0 not found, computing it
16/10/29 20:12:52 INFO CacheManager: Partition rdd_71_1 not found, computing it
16/10/29 20:12:52 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12565ea252.csv:0+6784485
16/10/29 20:12:52 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a12565ea252.csv:6784485+6784486
16/10/29 20:12:52 ERROR CsvRelation$: Exception while parsing line: 1,"What are some Caribbean cruises for October?","<p>My fiance and I are looking for a good Caribbean cruise in October and were wondering which islands are best to see and which Cruise line to take?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:52 ERROR Executor: Exception in task 1.0 in stage 14.0 (TID 15)
java.lang.NumberFormatException: For input string: "<img src=""http://i.stack.imgur.com/ihnR5.png"" alt=""map of the amazon""></p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:52 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:12:52 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 14)
java.lang.NumberFormatException: For input string: "<p>It seems like a lot of the cruises don't run in this month due to Hurricane season so I'm looking for other good options.</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:12:52 WARN TaskSetManager: Lost task 1.0 in stage 14.0 (TID 15, localhost): java.lang.NumberFormatException: For input string: "<img src=""http://i.stack.imgur.com/ihnR5.png"" alt=""map of the amazon""></p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:12:52 ERROR TaskSetManager: Task 1 in stage 14.0 failed 1 times; aborting job
16/10/29 20:12:52 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
16/10/29 20:12:52 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 14, localhost): java.lang.NumberFormatException: For input string: "<p>It seems like a lot of the cruises don't run in this month due to Hurricane season so I'm looking for other good options.</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:12:52 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
16/10/29 20:12:52 INFO TaskSchedulerImpl: Cancelling stage 14
16/10/29 20:12:52 INFO DAGScheduler: ShuffleMapStage 14 (sql at NativeMethodAccessorImpl.java:-2) failed in 0,018 s
16/10/29 20:12:52 INFO DAGScheduler: Job 12 failed: sql at NativeMethodAccessorImpl.java:-2, took 0,040099 s
16/10/29 20:13:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:13:00 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:13:00 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:13:00 INFO DAGScheduler: Got job 13 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:13:00 INFO DAGScheduler: Final stage: ResultStage 16 (collect at utils.scala:59)
16/10/29 20:13:00 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:13:00 INFO DAGScheduler: Missing parents: List()
16/10/29 20:13:00 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[81] at map at utils.scala:56), which has no missing parents
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 5.4 KB, free 1239.4 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1242.4 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[81] at map at utils.scala:56)
16/10/29 20:13:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
16/10/29 20:13:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, partition 0,PROCESS_LOCAL, 2797 bytes)
16/10/29 20:13:00 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
16/10/29 20:13:00 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1108 bytes result sent to driver
16/10/29 20:13:00 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 6 ms on localhost (1/1)
16/10/29 20:13:00 INFO DAGScheduler: ResultStage 16 (collect at utils.scala:59) finished in 0,006 s
16/10/29 20:13:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
16/10/29 20:13:00 INFO DAGScheduler: Job 13 finished: collect at utils.scala:59, took 0,015481 s
16/10/29 20:13:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:13:00 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:13:00 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:13:00 INFO DAGScheduler: Got job 14 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:13:00 INFO DAGScheduler: Final stage: ResultStage 17 (collect at utils.scala:59)
16/10/29 20:13:00 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:13:00 INFO DAGScheduler: Missing parents: List()
16/10/29 20:13:00 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[85] at map at utils.scala:56), which has no missing parents
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.4 KB, free 1247.8 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1250.8 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[85] at map at utils.scala:56)
16/10/29 20:13:00 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
16/10/29 20:13:00 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, partition 0,PROCESS_LOCAL, 2797 bytes)
16/10/29 20:13:00 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
16/10/29 20:13:00 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 1108 bytes result sent to driver
16/10/29 20:13:00 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 5 ms on localhost (1/1)
16/10/29 20:13:00 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
16/10/29 20:13:00 INFO DAGScheduler: ResultStage 17 (collect at utils.scala:59) finished in 0,008 s
16/10/29 20:13:00 INFO DAGScheduler: Job 14 finished: collect at utils.scala:59, took 0,020274 s
16/10/29 20:13:00 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:13:00 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 208.5 KB, free 1459.3 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.3 KB, free 1478.6 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 21 from textFile at TextFile.scala:30
16/10/29 20:13:00 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:13:00 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/10/29 20:13:00 INFO DAGScheduler: Got job 15 (take at CsvRelation.scala:249) with 1 output partitions
16/10/29 20:13:00 INFO DAGScheduler: Final stage: ResultStage 18 (take at CsvRelation.scala:249)
16/10/29 20:13:00 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:13:00 INFO DAGScheduler: Missing parents: List()
16/10/29 20:13:00 INFO DAGScheduler: Submitting ResultStage 18 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a121ab98508.csv MapPartitionsRDD[88] at textFile at TextFile.scala:30), which has no missing parents
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 3.1 KB, free 1481.8 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1900.0 B, free 1483.6 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:58858 (size: 1900.0 B, free: 511.0 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a121ab98508.csv MapPartitionsRDD[88] at textFile at TextFile.scala:30)
16/10/29 20:13:00 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
16/10/29 20:13:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:13:00 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
16/10/29 20:13:00 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a121ab98508.csv:0+967565
16/10/29 20:13:00 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2254 bytes result sent to driver
16/10/29 20:13:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 6 ms on localhost (1/1)
16/10/29 20:13:00 INFO DAGScheduler: ResultStage 18 (take at CsvRelation.scala:249) finished in 0,007 s
16/10/29 20:13:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
16/10/29 20:13:00 INFO DAGScheduler: Job 15 finished: take at CsvRelation.scala:249, took 0,016557 s
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 208.5 KB, free 1692.1 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.3 KB, free 1711.5 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 23 from textFile at TextFile.scala:30
16/10/29 20:13:00 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:13:00 INFO SparkContext: Starting job: sql at null:-1
16/10/29 20:13:00 INFO DAGScheduler: Registering RDD 98 (sql at null:-1)
16/10/29 20:13:00 INFO DAGScheduler: Got job 16 (sql at null:-1) with 1 output partitions
16/10/29 20:13:00 INFO DAGScheduler: Final stage: ResultStage 20 (sql at null:-1)
16/10/29 20:13:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
16/10/29 20:13:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
16/10/29 20:13:00 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[98] at sql at null:-1), which has no missing parents
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 16.1 KB, free 1727.6 KB)
16/10/29 20:13:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1735.6 KB)
16/10/29 20:13:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:58858 (size: 8.0 KB, free: 510.9 MB)
16/10/29 20:13:00 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[98] at sql at null:-1)
16/10/29 20:13:00 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
16/10/29 20:13:00 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:13:00 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 20, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:13:00 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
16/10/29 20:13:00 INFO Executor: Running task 1.0 in stage 19.0 (TID 20)
16/10/29 20:13:00 INFO CacheManager: Partition rdd_95_1 not found, computing it
16/10/29 20:13:00 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a121ab98508.csv:967565+967566
16/10/29 20:13:00 INFO CacheManager: Partition rdd_95_0 not found, computing it
16/10/29 20:13:00 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a121ab98508.csv:0+967565
16/10/29 20:13:01 INFO GenerateUnsafeProjection: Code generated in 16.625574 ms
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 43
16/10/29 20:13:01 INFO BlockManager: Removing RDD 19
16/10/29 20:13:01 INFO ContextCleaner: Cleaned RDD 19
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 7
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:58858 in memory (size: 1900.0 B, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 53
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 52
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 51
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 50
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 49
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:58858 in memory (size: 8.3 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 48
16/10/29 20:13:01 INFO ContextCleaner: Cleaned shuffle 2
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 47
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 46
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 45
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 44
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 42
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 41
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 40
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:58858 in memory (size: 1903.0 B, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 38
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 37
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 36
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 35
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 34
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:58858 in memory (size: 8.3 KB, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 33
16/10/29 20:13:01 INFO ContextCleaner: Cleaned shuffle 1
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 32
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 31
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 30
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 29
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 28
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 27
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 26
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 25
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:58858 in memory (size: 1902.0 B, free: 511.0 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 23
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 511.1 MB)
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 22
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 21
16/10/29 20:13:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 511.1 MB)
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 20
16/10/29 20:13:01 INFO ContextCleaner: Cleaned accumulator 19
16/10/29 20:13:02 INFO MemoryStore: Block rdd_95_0 stored as values in memory (estimated size 42.9 KB, free 750.5 KB)
16/10/29 20:13:02 INFO BlockManagerInfo: Added rdd_95_0 in memory on localhost:58858 (size: 42.9 KB, free: 511.0 MB)
16/10/29 20:13:02 INFO MemoryStore: Block rdd_95_1 stored as values in memory (estimated size 41.6 KB, free 792.1 KB)
16/10/29 20:13:02 INFO BlockManagerInfo: Added rdd_95_1 in memory on localhost:58858 (size: 41.6 KB, free: 511.0 MB)
16/10/29 20:13:02 INFO GeneratePredicate: Code generated in 5.279014 ms
16/10/29 20:13:02 INFO GenerateColumnAccessor: Code generated in 24.498063 ms
16/10/29 20:13:02 INFO GenerateMutableProjection: Code generated in 10.221083 ms
16/10/29 20:13:02 INFO GenerateUnsafeProjection: Code generated in 10.129375 ms
16/10/29 20:13:02 INFO GenerateMutableProjection: Code generated in 16.356897 ms
16/10/29 20:13:02 INFO GenerateUnsafeRowJoiner: Code generated in 6.059252 ms
16/10/29 20:13:02 INFO GenerateUnsafeProjection: Code generated in 6.952978 ms
16/10/29 20:13:02 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4181 bytes result sent to driver
16/10/29 20:13:02 INFO Executor: Finished task 1.0 in stage 19.0 (TID 20). 4186 bytes result sent to driver
16/10/29 20:13:02 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 20) in 1758 ms on localhost (1/2)
16/10/29 20:13:02 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 1759 ms on localhost (2/2)
16/10/29 20:13:02 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
16/10/29 20:13:02 INFO DAGScheduler: ShuffleMapStage 19 (sql at null:-1) finished in 1,760 s
16/10/29 20:13:02 INFO DAGScheduler: looking for newly runnable stages
16/10/29 20:13:02 INFO DAGScheduler: running: Set()
16/10/29 20:13:02 INFO DAGScheduler: waiting: Set(ResultStage 20)
16/10/29 20:13:02 INFO DAGScheduler: failed: Set()
16/10/29 20:13:02 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[101] at sql at null:-1), which has no missing parents
16/10/29 20:13:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 9.3 KB, free 801.4 KB)
16/10/29 20:13:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.6 KB, free 806.0 KB)
16/10/29 20:13:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:58858 (size: 4.6 KB, free: 511.0 MB)
16/10/29 20:13:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[101] at sql at null:-1)
16/10/29 20:13:02 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
16/10/29 20:13:02 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 21, localhost, partition 0,NODE_LOCAL, 2290 bytes)
16/10/29 20:13:02 INFO Executor: Running task 0.0 in stage 20.0 (TID 21)
16/10/29 20:13:02 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/10/29 20:13:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
16/10/29 20:13:02 INFO GenerateMutableProjection: Code generated in 5.04527 ms
16/10/29 20:13:02 INFO GenerateMutableProjection: Code generated in 7.310847 ms
16/10/29 20:13:02 INFO Executor: Finished task 0.0 in stage 20.0 (TID 21). 1830 bytes result sent to driver
16/10/29 20:13:02 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 21) in 96 ms on localhost (1/1)
16/10/29 20:13:02 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
16/10/29 20:13:02 INFO DAGScheduler: ResultStage 20 (sql at null:-1) finished in 0,096 s
16/10/29 20:13:02 INFO DAGScheduler: Job 16 finished: sql at null:-1, took 1,884161 s
16/10/29 20:13:02 INFO ParseDriver: Parsing command: SELECT count(*) FROM `sample_submission`
16/10/29 20:13:04 INFO ParseDriver: Parse Completed
16/10/29 20:13:04 INFO SparkContext: Starting job: collect at utils.scala:181
16/10/29 20:13:04 INFO DAGScheduler: Registering RDD 105 (collect at utils.scala:181)
16/10/29 20:13:04 INFO DAGScheduler: Got job 17 (collect at utils.scala:181) with 1 output partitions
16/10/29 20:13:04 INFO DAGScheduler: Final stage: ResultStage 22 (collect at utils.scala:181)
16/10/29 20:13:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
16/10/29 20:13:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 21)
16/10/29 20:13:04 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[105] at collect at utils.scala:181), which has no missing parents
16/10/29 20:13:04 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 16.2 KB, free 822.2 KB)
16/10/29 20:13:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.0 KB, free 830.3 KB)
16/10/29 20:13:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:58858 (size: 8.0 KB, free: 511.0 MB)
16/10/29 20:13:04 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[105] at collect at utils.scala:181)
16/10/29 20:13:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks
16/10/29 20:13:04 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:13:04 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:13:04 INFO Executor: Running task 1.0 in stage 21.0 (TID 23)
16/10/29 20:13:04 INFO Executor: Running task 0.0 in stage 21.0 (TID 22)
16/10/29 20:13:04 INFO BlockManager: Found block rdd_95_1 locally
16/10/29 20:13:04 INFO BlockManager: Found block rdd_95_0 locally
16/10/29 20:13:04 INFO Executor: Finished task 0.0 in stage 21.0 (TID 22). 2679 bytes result sent to driver
16/10/29 20:13:04 INFO Executor: Finished task 1.0 in stage 21.0 (TID 23). 2679 bytes result sent to driver
16/10/29 20:13:04 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 23) in 22 ms on localhost (1/2)
16/10/29 20:13:04 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 22) in 23 ms on localhost (2/2)
16/10/29 20:13:04 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
16/10/29 20:13:04 INFO DAGScheduler: ShuffleMapStage 21 (collect at utils.scala:181) finished in 0,024 s
16/10/29 20:13:04 INFO DAGScheduler: looking for newly runnable stages
16/10/29 20:13:04 INFO DAGScheduler: running: Set()
16/10/29 20:13:04 INFO DAGScheduler: waiting: Set(ResultStage 22)
16/10/29 20:13:04 INFO DAGScheduler: failed: Set()
16/10/29 20:13:04 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[108] at collect at utils.scala:181), which has no missing parents
16/10/29 20:13:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.4 KB, free 839.7 KB)
16/10/29 20:13:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.7 KB, free 844.3 KB)
16/10/29 20:13:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:58858 (size: 4.7 KB, free: 511.0 MB)
16/10/29 20:13:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[108] at collect at utils.scala:181)
16/10/29 20:13:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
16/10/29 20:13:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 24, localhost, partition 0,NODE_LOCAL, 2290 bytes)
16/10/29 20:13:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 24)
16/10/29 20:13:04 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/10/29 20:13:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
16/10/29 20:13:04 INFO Executor: Finished task 0.0 in stage 22.0 (TID 24). 1830 bytes result sent to driver
16/10/29 20:13:04 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 24) in 10 ms on localhost (1/1)
16/10/29 20:13:04 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
16/10/29 20:13:04 INFO DAGScheduler: ResultStage 22 (collect at utils.scala:181) finished in 0,010 s
16/10/29 20:13:04 INFO DAGScheduler: Job 17 finished: collect at utils.scala:181, took 0,046360 s
16/10/29 20:13:04 INFO ParseDriver: Parsing command: SELECT *
FROM `sample_submission` AS `zzz1`
WHERE (0 = 1)
16/10/29 20:13:04 INFO ParseDriver: Parse Completed
16/10/29 20:13:04 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:13:04 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:13:11 INFO ParseDriver: Parsing command: SELECT * FROM biology LIMIT 1000
16/10/29 20:13:11 INFO ParseDriver: Parse Completed
16/10/29 20:13:11 INFO SparkContext: Starting job: collect at utils.scala:181
16/10/29 20:13:11 INFO DAGScheduler: Got job 18 (collect at utils.scala:181) with 1 output partitions
16/10/29 20:13:11 INFO DAGScheduler: Final stage: ResultStage 23 (collect at utils.scala:181)
16/10/29 20:13:11 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:13:11 INFO DAGScheduler: Missing parents: List()
16/10/29 20:13:11 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[112] at collect at utils.scala:181), which has no missing parents
16/10/29 20:13:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 13.1 KB, free 857.4 KB)
16/10/29 20:13:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.6 KB, free 864.0 KB)
16/10/29 20:13:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:58858 (size: 6.6 KB, free: 511.0 MB)
16/10/29 20:13:11 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
16/10/29 20:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[112] at collect at utils.scala:181)
16/10/29 20:13:11 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
16/10/29 20:13:11 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 25, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:13:11 INFO Executor: Running task 0.0 in stage 23.0 (TID 25)
16/10/29 20:13:11 INFO CacheManager: Partition rdd_47_0 not found, computing it
16/10/29 20:13:11 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a123581b437.csv:0+4925434
16/10/29 20:13:11 ERROR CsvRelation$: Exception while parsing line: 1,"What is the criticality of the ribosome binding site relative to the start codon in prokaryotic translation?","<p>In prokaryotic translation, how critical for efficient translation is the location of the ribosome binding site, relative to the start codon?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:13:11 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:13:11 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 25)
java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:13:11 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 25, localhost): java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:13:11 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job
16/10/29 20:13:11 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
16/10/29 20:13:11 INFO TaskSchedulerImpl: Cancelling stage 23
16/10/29 20:13:11 INFO DAGScheduler: ResultStage 23 (collect at utils.scala:181) failed in 0,019 s
16/10/29 20:13:11 INFO DAGScheduler: Job 18 failed: collect at utils.scala:181, took 0,032265 s
16/10/29 20:14:15 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:14:15 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:14:15 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:14:15 INFO DAGScheduler: Got job 19 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:14:15 INFO DAGScheduler: Final stage: ResultStage 24 (collect at utils.scala:59)
16/10/29 20:14:15 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:14:15 INFO DAGScheduler: Missing parents: List()
16/10/29 20:14:15 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[116] at map at utils.scala:56), which has no missing parents
16/10/29 20:14:15 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.4 KB, free 869.4 KB)
16/10/29 20:14:15 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.0 KB, free 872.4 KB)
16/10/29 20:14:15 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 511.0 MB)
16/10/29 20:14:15 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
16/10/29 20:14:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[116] at map at utils.scala:56)
16/10/29 20:14:15 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
16/10/29 20:14:15 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 26, localhost, partition 0,PROCESS_LOCAL, 2848 bytes)
16/10/29 20:14:15 INFO Executor: Running task 0.0 in stage 24.0 (TID 26)
16/10/29 20:14:15 INFO Executor: Finished task 0.0 in stage 24.0 (TID 26). 1128 bytes result sent to driver
16/10/29 20:14:15 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 26) in 8 ms on localhost (1/1)
16/10/29 20:14:15 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
16/10/29 20:14:15 INFO DAGScheduler: ResultStage 24 (collect at utils.scala:59) finished in 0,009 s
16/10/29 20:14:15 INFO DAGScheduler: Job 19 finished: collect at utils.scala:59, took 0,022023 s
16/10/29 20:14:24 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:14:24 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:14:24 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:14:24 INFO DAGScheduler: Got job 20 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:14:24 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:59)
16/10/29 20:14:24 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:14:24 INFO DAGScheduler: Missing parents: List()
16/10/29 20:14:24 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[120] at map at utils.scala:56), which has no missing parents
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 5.4 KB, free 877.9 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.0 KB, free 880.8 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
16/10/29 20:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[120] at map at utils.scala:56)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
16/10/29 20:14:24 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2848 bytes)
16/10/29 20:14:24 INFO Executor: Running task 0.0 in stage 25.0 (TID 27)
16/10/29 20:14:24 INFO Executor: Finished task 0.0 in stage 25.0 (TID 27). 1128 bytes result sent to driver
16/10/29 20:14:24 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 27) in 5 ms on localhost (1/1)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
16/10/29 20:14:24 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:59) finished in 0,006 s
16/10/29 20:14:24 INFO DAGScheduler: Job 20 finished: collect at utils.scala:59, took 0,019480 s
16/10/29 20:14:24 INFO MapPartitionsRDD: Removing RDD 47 from persistence list
16/10/29 20:14:24 INFO BlockManager: Removing RDD 47
16/10/29 20:14:24 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:14:24 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:14:24 INFO SparkContext: Starting job: collect at utils.scala:59
16/10/29 20:14:24 INFO DAGScheduler: Got job 21 (collect at utils.scala:59) with 1 output partitions
16/10/29 20:14:24 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:59)
16/10/29 20:14:24 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:14:24 INFO DAGScheduler: Missing parents: List()
16/10/29 20:14:24 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[124] at map at utils.scala:56), which has no missing parents
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.4 KB, free 886.3 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KB, free 889.2 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:58858 (size: 3.0 KB, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
16/10/29 20:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[124] at map at utils.scala:56)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
16/10/29 20:14:24 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 28, localhost, partition 0,PROCESS_LOCAL, 2807 bytes)
16/10/29 20:14:24 INFO Executor: Running task 0.0 in stage 26.0 (TID 28)
16/10/29 20:14:24 INFO Executor: Finished task 0.0 in stage 26.0 (TID 28). 1118 bytes result sent to driver
16/10/29 20:14:24 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 28) in 5 ms on localhost (1/1)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
16/10/29 20:14:24 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:59) finished in 0,005 s
16/10/29 20:14:24 INFO DAGScheduler: Job 21 finished: collect at utils.scala:59, took 0,015610 s
16/10/29 20:14:24 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/10/29 20:14:24 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 208.5 KB, free 1097.8 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 19.3 KB, free 1117.1 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 32 from textFile at TextFile.scala:30
16/10/29 20:14:24 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:14:24 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/10/29 20:14:24 INFO DAGScheduler: Got job 22 (take at CsvRelation.scala:249) with 1 output partitions
16/10/29 20:14:24 INFO DAGScheduler: Final stage: ResultStage 27 (take at CsvRelation.scala:249)
16/10/29 20:14:24 INFO DAGScheduler: Parents of final stage: List()
16/10/29 20:14:24 INFO DAGScheduler: Missing parents: List()
16/10/29 20:14:24 INFO DAGScheduler: Submitting ResultStage 27 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a1255133611.csv MapPartitionsRDD[127] at textFile at TextFile.scala:30), which has no missing parents
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 3.2 KB, free 1120.2 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 1907.0 B, free 1122.1 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:58858 (size: 1907.0 B, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
16/10/29 20:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//Rtmpeokyzg/file6a1255133611.csv MapPartitionsRDD[127] at textFile at TextFile.scala:30)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
16/10/29 20:14:24 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 29, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/10/29 20:14:24 INFO Executor: Running task 0.0 in stage 27.0 (TID 29)
16/10/29 20:14:24 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a1255133611.csv:0+4925434
16/10/29 20:14:24 INFO Executor: Finished task 0.0 in stage 27.0 (TID 29). 3021 bytes result sent to driver
16/10/29 20:14:24 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 29) in 11 ms on localhost (1/1)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
16/10/29 20:14:24 INFO DAGScheduler: ResultStage 27 (take at CsvRelation.scala:249) finished in 0,012 s
16/10/29 20:14:24 INFO DAGScheduler: Job 22 finished: take at CsvRelation.scala:249, took 0,021715 s
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 208.5 KB, free 1330.6 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.3 KB, free 1349.9 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:58858 (size: 19.3 KB, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 34 from textFile at TextFile.scala:30
16/10/29 20:14:24 INFO FileInputFormat: Total input paths to process : 1
16/10/29 20:14:24 INFO SparkContext: Starting job: sql at null:-1
16/10/29 20:14:24 INFO DAGScheduler: Registering RDD 137 (sql at null:-1)
16/10/29 20:14:24 INFO DAGScheduler: Got job 23 (sql at null:-1) with 1 output partitions
16/10/29 20:14:24 INFO DAGScheduler: Final stage: ResultStage 29 (sql at null:-1)
16/10/29 20:14:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
16/10/29 20:14:24 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
16/10/29 20:14:24 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[137] at sql at null:-1), which has no missing parents
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 17.3 KB, free 1367.2 KB)
16/10/29 20:14:24 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 8.3 KB, free 1375.5 KB)
16/10/29 20:14:24 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:58858 (size: 8.3 KB, free: 510.9 MB)
16/10/29 20:14:24 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
16/10/29 20:14:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[137] at sql at null:-1)
16/10/29 20:14:24 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
16/10/29 20:14:24 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 30, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:14:24 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 31, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/10/29 20:14:24 INFO Executor: Running task 0.0 in stage 28.0 (TID 30)
16/10/29 20:14:24 INFO Executor: Running task 1.0 in stage 28.0 (TID 31)
16/10/29 20:14:24 INFO CacheManager: Partition rdd_134_1 not found, computing it
16/10/29 20:14:24 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a1255133611.csv:4925434+4925435
16/10/29 20:14:24 INFO CacheManager: Partition rdd_134_0 not found, computing it
16/10/29 20:14:24 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/Rtmpeokyzg/file6a1255133611.csv:0+4925434
16/10/29 20:14:24 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:14:24 ERROR CsvRelation$: Exception while parsing line: 1,"What is the criticality of the ribosome binding site relative to the start codon in prokaryotic translation?","<p>In prokaryotic translation, how critical for efficient translation is the location of the ribosome binding site, relative to the start codon?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:14:24 WARN CsvRelation$: Ignoring empty line: 
16/10/29 20:14:24 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 30)
java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:14:24 ERROR Executor: Exception in task 1.0 in stage 28.0 (TID 31)
java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/29 20:14:24 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 30, localhost): java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:14:24 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job
16/10/29 20:14:24 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
16/10/29 20:14:24 WARN TaskSetManager: Lost task 1.0 in stage 28.0 (TID 31, localhost): java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/29 20:14:24 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
16/10/29 20:14:24 INFO TaskSchedulerImpl: Cancelling stage 28
16/10/29 20:14:24 INFO DAGScheduler: ShuffleMapStage 28 (sql at null:-1) failed in 0,026 s
16/10/29 20:14:24 INFO DAGScheduler: Job 23 failed: sql at null:-1, took 0,044518 s
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 77
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:58858 in memory (size: 8.3 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 92
16/10/29 23:22:23 INFO ContextCleaner: Cleaned shuffle 5
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 91
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 90
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 89
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 88
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 87
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 86
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 85
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 84
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:58858 in memory (size: 1907.0 B, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 82
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:58858 in memory (size: 19.3 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 81
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 80
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 79
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 78
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:58858 in memory (size: 3.0 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 76
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:58858 in memory (size: 6.6 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 75
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:58858 in memory (size: 4.7 KB, free: 510.9 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 74
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:58858 in memory (size: 8.0 KB, free: 511.0 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 73
16/10/29 23:22:23 INFO ContextCleaner: Cleaned shuffle 4
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:58858 in memory (size: 4.6 KB, free: 511.0 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 64
16/10/29 23:22:23 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:58858 in memory (size: 8.0 KB, free: 511.0 MB)
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 63
16/10/29 23:22:23 INFO ContextCleaner: Cleaned shuffle 3
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 62
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 61
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 60
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 59
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 58
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 57
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 56
16/10/29 23:22:23 INFO ContextCleaner: Cleaned accumulator 55
