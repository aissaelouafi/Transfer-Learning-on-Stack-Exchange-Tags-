16/11/01 17:27:41 INFO SparkContext: Running Spark version 1.6.2
16/11/01 17:27:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/11/01 17:27:42 INFO SecurityManager: Changing view acls to: aissaelouafi
16/11/01 17:27:42 INFO SecurityManager: Changing modify acls to: aissaelouafi
16/11/01 17:27:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(aissaelouafi); users with modify permissions: Set(aissaelouafi)
16/11/01 17:27:42 INFO Utils: Successfully started service 'sparkDriver' on port 53761.
16/11/01 17:27:43 INFO Slf4jLogger: Slf4jLogger started
16/11/01 17:27:43 INFO Remoting: Starting remoting
16/11/01 17:27:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:53764]
16/11/01 17:27:43 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 53764.
16/11/01 17:27:43 INFO SparkEnv: Registering MapOutputTracker
16/11/01 17:27:43 INFO SparkEnv: Registering BlockManagerMaster
16/11/01 17:27:43 INFO DiskBlockManager: Created local directory at /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/blockmgr-4ea75a0f-e32f-4a7d-9e69-610ea94057c1
16/11/01 17:27:43 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
16/11/01 17:27:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/11/01 17:27:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/11/01 17:27:44 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
16/11/01 17:27:44 INFO HttpFileServer: HTTP File server directory is /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/httpd-e60c9b22-a0b9-4fae-bcb9-edb78a60dec2
16/11/01 17:27:44 INFO HttpServer: Starting HTTP Server
16/11/01 17:27:44 INFO Utils: Successfully started service 'HTTP file server' on port 53765.
16/11/01 17:27:44 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/com.databricks_spark-csv_2.11-1.3.0.jar at http://127.0.0.1:53765/jars/com.databricks_spark-csv_2.11-1.3.0.jar with timestamp 1478017664200
16/11/01 17:27:44 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar at http://127.0.0.1:53765/jars/org.apache.commons_commons-csv-1.1.jar with timestamp 1478017664203
16/11/01 17:27:44 INFO SparkContext: Added JAR file:/Users/aissaelouafi/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar at http://127.0.0.1:53765/jars/com.univocity_univocity-parsers-1.5.1.jar with timestamp 1478017664207
16/11/01 17:27:44 INFO SparkContext: Added JAR file:/usr/local/lib/R/3.2/site-library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:53765/jars/sparklyr-1.6-2.10.jar with timestamp 1478017664212
16/11/01 17:27:44 INFO Executor: Starting executor ID driver on host localhost
16/11/01 17:27:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53766.
16/11/01 17:27:44 INFO NettyBlockTransferService: Server created on 53766
16/11/01 17:27:44 INFO BlockManagerMaster: Trying to register BlockManager
16/11/01 17:27:44 INFO BlockManagerMasterEndpoint: Registering block manager localhost:53766 with 511.1 MB RAM, BlockManagerId(driver, localhost, 53766)
16/11/01 17:27:44 INFO BlockManagerMaster: Registered BlockManager
16/11/01 17:27:46 INFO HiveContext: Initializing execution hive, version 1.2.1
16/11/01 17:27:46 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
16/11/01 17:27:46 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/11/01 17:27:46 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/11/01 17:27:46 INFO ObjectStore: ObjectStore, initialize called
16/11/01 17:27:46 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/11/01 17:27:46 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/11/01 17:27:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/11/01 17:27:47 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/11/01 17:27:49 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/11/01 17:27:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:51 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/11/01 17:27:51 INFO ObjectStore: Initialized ObjectStore
16/11/01 17:27:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/11/01 17:27:52 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/11/01 17:27:52 INFO HiveMetaStore: Added admin role in metastore
16/11/01 17:27:52 INFO HiveMetaStore: Added public role in metastore
16/11/01 17:27:52 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/11/01 17:27:52 INFO HiveMetaStore: 0: get_all_databases
16/11/01 17:27:52 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_all_databases	
16/11/01 17:27:52 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/11/01 17:27:52 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/11/01 17:27:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:52 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/abe685ae-f0bb-45b5-95a3-f970db0a1959_resources
16/11/01 17:27:52 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/abe685ae-f0bb-45b5-95a3-f970db0a1959
16/11/01 17:27:52 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/aissaelouafi/abe685ae-f0bb-45b5-95a3-f970db0a1959
16/11/01 17:27:52 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/abe685ae-f0bb-45b5-95a3-f970db0a1959/_tmp_space.db
16/11/01 17:27:52 INFO HiveContext: default warehouse location is /user/hive/warehouse
16/11/01 17:27:52 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/11/01 17:27:52 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
16/11/01 17:27:52 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/11/01 17:27:53 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/11/01 17:27:53 INFO ObjectStore: ObjectStore, initialize called
16/11/01 17:27:53 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/11/01 17:27:53 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/11/01 17:27:53 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/11/01 17:27:53 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/11/01 17:27:54 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/11/01 17:27:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/11/01 17:27:56 INFO ObjectStore: Initialized ObjectStore
16/11/01 17:27:56 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/11/01 17:27:57 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
16/11/01 17:27:57 INFO HiveMetaStore: Added admin role in metastore
16/11/01 17:27:57 INFO HiveMetaStore: Added public role in metastore
16/11/01 17:27:57 INFO HiveMetaStore: No user is added in admin role, since config is empty
16/11/01 17:27:57 INFO HiveMetaStore: 0: get_all_databases
16/11/01 17:27:57 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_all_databases	
16/11/01 17:27:57 INFO HiveMetaStore: 0: get_functions: db=default pat=*
16/11/01 17:27:57 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/11/01 17:27:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/11/01 17:27:57 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/1a91f742-fdbc-4fee-842d-60238906be17_resources
16/11/01 17:27:57 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/1a91f742-fdbc-4fee-842d-60238906be17
16/11/01 17:27:57 INFO SessionState: Created local directory: /var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/aissaelouafi/1a91f742-fdbc-4fee-842d-60238906be17
16/11/01 17:27:57 INFO SessionState: Created HDFS directory: /tmp/hive/aissaelouafi/1a91f742-fdbc-4fee-842d-60238906be17/_tmp_space.db
16/11/01 17:28:35 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:28:35 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:28:35 INFO SparkContext: Starting job: collect at utils.scala:59
16/11/01 17:28:35 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
16/11/01 17:28:35 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
16/11/01 17:28:35 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:28:35 INFO DAGScheduler: Missing parents: List()
16/11/01 17:28:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
16/11/01 17:28:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
16/11/01 17:28:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
16/11/01 17:28:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:53766 (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:28:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
16/11/01 17:28:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
16/11/01 17:28:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/11/01 17:28:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2396 bytes)
16/11/01 17:28:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/11/01 17:28:36 INFO Executor: Fetching http://127.0.0.1:53765/jars/sparklyr-1.6-2.10.jar with timestamp 1478017664212
16/11/01 17:28:36 INFO Utils: Fetching http://127.0.0.1:53765/jars/sparklyr-1.6-2.10.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/fetchFileTemp3592722533782122958.tmp
16/11/01 17:28:36 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/sparklyr-1.6-2.10.jar to class loader
16/11/01 17:28:36 INFO Executor: Fetching http://127.0.0.1:53765/jars/org.apache.commons_commons-csv-1.1.jar with timestamp 1478017664203
16/11/01 17:28:36 INFO Utils: Fetching http://127.0.0.1:53765/jars/org.apache.commons_commons-csv-1.1.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/fetchFileTemp381713202160682784.tmp
16/11/01 17:28:36 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/org.apache.commons_commons-csv-1.1.jar to class loader
16/11/01 17:28:36 INFO Executor: Fetching http://127.0.0.1:53765/jars/com.databricks_spark-csv_2.11-1.3.0.jar with timestamp 1478017664200
16/11/01 17:28:36 INFO Utils: Fetching http://127.0.0.1:53765/jars/com.databricks_spark-csv_2.11-1.3.0.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/fetchFileTemp1898314016943040072.tmp
16/11/01 17:28:36 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/com.databricks_spark-csv_2.11-1.3.0.jar to class loader
16/11/01 17:28:36 INFO Executor: Fetching http://127.0.0.1:53765/jars/com.univocity_univocity-parsers-1.5.1.jar with timestamp 1478017664207
16/11/01 17:28:36 INFO Utils: Fetching http://127.0.0.1:53765/jars/com.univocity_univocity-parsers-1.5.1.jar to /private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/fetchFileTemp6808642870457026572.tmp
16/11/01 17:28:36 INFO Executor: Adding file:/private/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/spark-ff1691d1-664d-4619-bf12-cb81822a0111/userFiles-71beb28f-f555-46df-8a2d-76a262a0044c/com.univocity_univocity-parsers-1.5.1.jar to class loader
16/11/01 17:28:36 INFO GenerateUnsafeProjection: Code generated in 196.305445 ms
16/11/01 17:28:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
16/11/01 17:28:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 496 ms on localhost (1/1)
16/11/01 17:28:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/11/01 17:28:36 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0,534 s
16/11/01 17:28:36 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0,847217 s
16/11/01 17:28:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:28:36 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
16/11/01 17:28:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:53766 (size: 19.3 KB, free: 511.1 MB)
16/11/01 17:28:37 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
16/11/01 17:28:37 INFO FileInputFormat: Total input paths to process : 1
16/11/01 17:28:37 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/11/01 17:28:37 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
16/11/01 17:28:37 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
16/11/01 17:28:37 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:28:37 INFO DAGScheduler: Missing parents: List()
16/11/01 17:28:37 INFO DAGScheduler: Submitting ResultStage 1 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//RtmpUF2Ddt/file742f680c8c06.csv MapPartitionsRDD[6] at textFile at TextFile.scala:30), which has no missing parents
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 92.7 KB)
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1902.0 B, free 94.5 KB)
16/11/01 17:28:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:53766 (size: 1902.0 B, free: 511.1 MB)
16/11/01 17:28:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
16/11/01 17:28:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//RtmpUF2Ddt/file742f680c8c06.csv MapPartitionsRDD[6] at textFile at TextFile.scala:30)
16/11/01 17:28:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/11/01 17:28:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/11/01 17:28:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
16/11/01 17:28:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f680c8c06.csv:0+4925434
16/11/01 17:28:37 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/11/01 17:28:37 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/11/01 17:28:37 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/11/01 17:28:37 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/11/01 17:28:37 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/11/01 17:28:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3021 bytes result sent to driver
16/11/01 17:28:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 99 ms on localhost (1/1)
16/11/01 17:28:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/11/01 17:28:37 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0,100 s
16/11/01 17:28:37 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0,127101 s
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.0 KB)
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.4 KB)
16/11/01 17:28:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:53766 (size: 19.3 KB, free: 511.1 MB)
16/11/01 17:28:37 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
16/11/01 17:28:37 INFO FileInputFormat: Total input paths to process : 1
16/11/01 17:28:37 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
16/11/01 17:28:37 INFO DAGScheduler: Registering RDD 16 (sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:28:37 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/11/01 17:28:37 INFO DAGScheduler: Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:28:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
16/11/01 17:28:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
16/11/01 17:28:37 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 17.3 KB, free 339.6 KB)
16/11/01 17:28:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.3 KB, free 348.0 KB)
16/11/01 17:28:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:53766 (size: 8.3 KB, free: 511.1 MB)
16/11/01 17:28:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
16/11/01 17:28:37 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:28:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
16/11/01 17:28:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:28:37 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:28:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
16/11/01 17:28:37 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
16/11/01 17:28:37 INFO CacheManager: Partition rdd_13_0 not found, computing it
16/11/01 17:28:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f680c8c06.csv:0+4925434
16/11/01 17:28:37 INFO CacheManager: Partition rdd_13_1 not found, computing it
16/11/01 17:28:37 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f680c8c06.csv:4925434+4925435
16/11/01 17:28:37 INFO GenerateUnsafeProjection: Code generated in 19.207081 ms
16/11/01 17:28:37 WARN CsvRelation$: Ignoring empty line: 
16/11/01 17:28:37 ERROR CsvRelation$: Exception while parsing line: 1,"What is the criticality of the ribosome binding site relative to the start codon in prokaryotic translation?","<p>In prokaryotic translation, how critical for efficient translation is the location of the ribosome binding site, relative to the start codon?</p>. 
java.io.IOException: (startline 1) EOF reached before encapsulated token finished
	at org.apache.commons.csv.Lexer.parseEncapsulatedToken(Lexer.java:282)
	at org.apache.commons.csv.Lexer.nextToken(Lexer.java:152)
	at org.apache.commons.csv.CSVParser.nextRecord(CSVParser.java:498)
	at org.apache.commons.csv.CSVParser.getRecords(CSVParser.java:365)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:282)
	at com.databricks.spark.csv.CsvRelation$$anonfun$com$databricks$spark$csv$CsvRelation$$parseCSV$1.apply(CsvRelation.scala:280)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/01 17:28:37 WARN CsvRelation$: Ignoring empty line: 
16/11/01 17:28:38 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 3)
java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/01 17:28:38 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/11/01 17:28:38 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NumberFormatException: For input string: "<p>Ideally"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/11/01 17:28:38 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
16/11/01 17:28:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/11/01 17:28:38 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 3, localhost): java.lang.NumberFormatException: For input string: "<p>I think the main issue is regulatory? Perhaps a lack of precedents in this area. What do you think of my analysis? Is this idea even feasible?</p>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:53)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:118)
	at com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$2.apply(CsvRelation.scala:104)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$3$$anon$1.hasNext(InMemoryColumnarTableScan.scala:169)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:284)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/11/01 17:28:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/11/01 17:28:38 INFO TaskSchedulerImpl: Cancelling stage 2
16/11/01 17:28:38 INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:-2) failed in 0,168 s
16/11/01 17:28:38 INFO DAGScheduler: Job 2 failed: sql at NativeMethodAccessorImpl.java:-2, took 0,227187 s
16/11/01 17:28:39 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:28:39 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:29:45 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:29:45 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:29:45 INFO SparkContext: Starting job: collect at utils.scala:59
16/11/01 17:29:45 INFO DAGScheduler: Got job 3 (collect at utils.scala:59) with 1 output partitions
16/11/01 17:29:45 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:59)
16/11/01 17:29:45 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:29:45 INFO DAGScheduler: Missing parents: List()
16/11/01 17:29:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[24] at map at utils.scala:56), which has no missing parents
16/11/01 17:29:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.4 KB, free 353.4 KB)
16/11/01 17:29:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KB, free 356.4 KB)
16/11/01 17:29:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:53766 (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:29:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
16/11/01 17:29:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[24] at map at utils.scala:56)
16/11/01 17:29:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
16/11/01 17:29:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2700 bytes)
16/11/01 17:29:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
16/11/01 17:29:45 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1073 bytes result sent to driver
16/11/01 17:29:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 22 ms on localhost (1/1)
16/11/01 17:29:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
16/11/01 17:29:45 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:59) finished in 0,022 s
16/11/01 17:29:45 INFO DAGScheduler: Job 3 finished: collect at utils.scala:59, took 0,043605 s
16/11/01 17:31:36 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:31:36 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:31:36 INFO SparkContext: Starting job: collect at utils.scala:59
16/11/01 17:31:36 INFO DAGScheduler: Got job 4 (collect at utils.scala:59) with 1 output partitions
16/11/01 17:31:36 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:59)
16/11/01 17:31:36 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:31:36 INFO DAGScheduler: Missing parents: List()
16/11/01 17:31:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at map at utils.scala:56), which has no missing parents
16/11/01 17:31:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.4 KB, free 361.8 KB)
16/11/01 17:31:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 364.8 KB)
16/11/01 17:31:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:53766 (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:31:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
16/11/01 17:31:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at map at utils.scala:56)
16/11/01 17:31:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
16/11/01 17:31:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2700 bytes)
16/11/01 17:31:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
16/11/01 17:31:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1073 bytes result sent to driver
16/11/01 17:31:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 16 ms on localhost (1/1)
16/11/01 17:31:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
16/11/01 17:31:36 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:59) finished in 0,018 s
16/11/01 17:31:36 INFO DAGScheduler: Job 4 finished: collect at utils.scala:59, took 0,039778 s
16/11/01 17:37:43 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:37:43 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:37:43 INFO SparkContext: Starting job: collect at utils.scala:59
16/11/01 17:37:43 INFO DAGScheduler: Got job 5 (collect at utils.scala:59) with 1 output partitions
16/11/01 17:37:43 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:59)
16/11/01 17:37:43 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:37:43 INFO DAGScheduler: Missing parents: List()
16/11/01 17:37:43 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[32] at map at utils.scala:56), which has no missing parents
16/11/01 17:37:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.4 KB, free 370.2 KB)
16/11/01 17:37:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KB, free 373.2 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:53766 (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[32] at map at utils.scala:56)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
16/11/01 17:37:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2700 bytes)
16/11/01 17:37:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
16/11/01 17:37:44 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1073 bytes result sent to driver
16/11/01 17:37:44 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 20 ms on localhost (1/1)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
16/11/01 17:37:44 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:59) finished in 0,021 s
16/11/01 17:37:44 INFO DAGScheduler: Job 5 finished: collect at utils.scala:59, took 0,049298 s
16/11/01 17:37:44 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:37:44 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 208.5 KB, free 581.7 KB)
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.3 KB, free 601.0 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:53766 (size: 19.3 KB, free: 511.0 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 8 from textFile at TextFile.scala:30
16/11/01 17:37:44 INFO FileInputFormat: Total input paths to process : 1
16/11/01 17:37:44 INFO SparkContext: Starting job: take at CsvRelation.scala:249
16/11/01 17:37:44 INFO DAGScheduler: Got job 6 (take at CsvRelation.scala:249) with 1 output partitions
16/11/01 17:37:44 INFO DAGScheduler: Final stage: ResultStage 7 (take at CsvRelation.scala:249)
16/11/01 17:37:44 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:37:44 INFO DAGScheduler: Missing parents: List()
16/11/01 17:37:44 INFO DAGScheduler: Submitting ResultStage 7 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//RtmpUF2Ddt/file742f49da6ee1.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30), which has no missing parents
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 604.2 KB)
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1900.0 B, free 606.0 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:53766 (size: 1900.0 B, free: 511.0 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T//RtmpUF2Ddt/file742f49da6ee1.csv MapPartitionsRDD[35] at textFile at TextFile.scala:30)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
16/11/01 17:37:44 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/11/01 17:37:44 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
16/11/01 17:37:44 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f49da6ee1.csv:0+164
16/11/01 17:37:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2226 bytes result sent to driver
16/11/01 17:37:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 14 ms on localhost (1/1)
16/11/01 17:37:44 INFO DAGScheduler: ResultStage 7 (take at CsvRelation.scala:249) finished in 0,014 s
16/11/01 17:37:44 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
16/11/01 17:37:44 INFO DAGScheduler: Job 6 finished: take at CsvRelation.scala:249, took 0,027302 s
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 208.5 KB, free 814.5 KB)
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 19.3 KB, free 833.8 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:53766 (size: 19.3 KB, free: 511.0 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 10 from textFile at TextFile.scala:30
16/11/01 17:37:44 INFO FileInputFormat: Total input paths to process : 1
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 12
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 11
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 10
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 9
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 8
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 7
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 6
16/11/01 17:37:44 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
16/11/01 17:37:44 INFO DAGScheduler: Registering RDD 45 (sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:37:44 INFO DAGScheduler: Got job 7 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
16/11/01 17:37:44 INFO DAGScheduler: Final stage: ResultStage 9 (sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:37:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
16/11/01 17:37:44 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)
16/11/01 17:37:44 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[45] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 16.2 KB, free 850.0 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:53766 in memory (size: 1902.0 B, free: 511.0 MB)
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.0 KB, free 856.1 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:53766 (size: 8.0 KB, free: 511.0 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:44 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[45] at sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
16/11/01 17:37:44 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:37:44 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 9, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:37:44 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
16/11/01 17:37:44 INFO Executor: Running task 1.0 in stage 8.0 (TID 9)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 4
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:53766 in memory (size: 19.3 KB, free: 511.0 MB)
16/11/01 17:37:44 INFO CacheManager: Partition rdd_42_0 not found, computing it
16/11/01 17:37:44 INFO CacheManager: Partition rdd_42_1 not found, computing it
16/11/01 17:37:44 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f49da6ee1.csv:0+164
16/11/01 17:37:44 INFO HadoopRDD: Input split: file:/var/folders/m1/w_x_s7_n40lg7jx2v4fbsldc0000gn/T/RtmpUF2Ddt/file742f49da6ee1.csv:164+164
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:53766 in memory (size: 3.0 KB, free: 511.0 MB)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 3
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 2
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:53766 in memory (size: 1900.0 B, free: 511.0 MB)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 21
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:53766 in memory (size: 19.3 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:53766 in memory (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO GenerateUnsafeProjection: Code generated in 19.595878 ms
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 20
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 19
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:53766 in memory (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 18
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 17
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:53766 in memory (size: 3.0 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 16
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 15
16/11/01 17:37:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:53766 in memory (size: 8.3 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 14
16/11/01 17:37:44 INFO ContextCleaner: Cleaned shuffle 0
16/11/01 17:37:44 INFO ContextCleaner: Cleaned accumulator 13
16/11/01 17:37:44 INFO MemoryStore: Block rdd_42_1 stored as values in memory (estimated size 560.0 B, free 480.4 KB)
16/11/01 17:37:44 INFO MemoryStore: Block rdd_42_0 stored as values in memory (estimated size 584.0 B, free 480.9 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added rdd_42_1 in memory on localhost:53766 (size: 560.0 B, free: 511.1 MB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added rdd_42_0 in memory on localhost:53766 (size: 584.0 B, free: 511.1 MB)
16/11/01 17:37:44 INFO GeneratePredicate: Code generated in 5.318813 ms
16/11/01 17:37:44 INFO GenerateColumnAccessor: Code generated in 29.266067 ms
16/11/01 17:37:44 INFO GenerateMutableProjection: Code generated in 10.039098 ms
16/11/01 17:37:44 INFO GenerateUnsafeProjection: Code generated in 8.588795 ms
16/11/01 17:37:44 INFO GenerateMutableProjection: Code generated in 14.699007 ms
16/11/01 17:37:44 INFO GenerateUnsafeRowJoiner: Code generated in 11.95616 ms
16/11/01 17:37:44 INFO GenerateUnsafeProjection: Code generated in 10.684215 ms
16/11/01 17:37:44 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3580 bytes result sent to driver
16/11/01 17:37:44 INFO Executor: Finished task 1.0 in stage 8.0 (TID 9). 3578 bytes result sent to driver
16/11/01 17:37:44 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 9) in 414 ms on localhost (1/2)
16/11/01 17:37:44 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 415 ms on localhost (2/2)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
16/11/01 17:37:44 INFO DAGScheduler: ShuffleMapStage 8 (sql at NativeMethodAccessorImpl.java:-2) finished in 0,418 s
16/11/01 17:37:44 INFO DAGScheduler: looking for newly runnable stages
16/11/01 17:37:44 INFO DAGScheduler: running: Set()
16/11/01 17:37:44 INFO DAGScheduler: waiting: Set(ResultStage 9)
16/11/01 17:37:44 INFO DAGScheduler: failed: Set()
16/11/01 17:37:44 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[48] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.3 KB, free 490.2 KB)
16/11/01 17:37:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 494.8 KB)
16/11/01 17:37:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:53766 (size: 4.6 KB, free: 511.1 MB)
16/11/01 17:37:44 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[48] at sql at NativeMethodAccessorImpl.java:-2)
16/11/01 17:37:44 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
16/11/01 17:37:44 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 10, localhost, partition 0,NODE_LOCAL, 2290 bytes)
16/11/01 17:37:44 INFO Executor: Running task 0.0 in stage 9.0 (TID 10)
16/11/01 17:37:44 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/11/01 17:37:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
16/11/01 17:37:44 INFO GenerateMutableProjection: Code generated in 8.429909 ms
16/11/01 17:37:45 INFO GenerateMutableProjection: Code generated in 8.789197 ms
16/11/01 17:37:45 INFO Executor: Finished task 0.0 in stage 9.0 (TID 10). 1830 bytes result sent to driver
16/11/01 17:37:45 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 10) in 165 ms on localhost (1/1)
16/11/01 17:37:45 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
16/11/01 17:37:45 INFO DAGScheduler: ResultStage 9 (sql at NativeMethodAccessorImpl.java:-2) finished in 0,166 s
16/11/01 17:37:45 INFO DAGScheduler: Job 7 finished: sql at NativeMethodAccessorImpl.java:-2, took 0,636563 s
16/11/01 17:37:45 INFO ParseDriver: Parsing command: SELECT count(*) FROM `travel_tags`
16/11/01 17:37:46 INFO ParseDriver: Parse Completed
16/11/01 17:37:46 INFO SparkContext: Starting job: collect at utils.scala:181
16/11/01 17:37:46 INFO DAGScheduler: Registering RDD 52 (collect at utils.scala:181)
16/11/01 17:37:46 INFO DAGScheduler: Got job 8 (collect at utils.scala:181) with 1 output partitions
16/11/01 17:37:46 INFO DAGScheduler: Final stage: ResultStage 11 (collect at utils.scala:181)
16/11/01 17:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
16/11/01 17:37:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)
16/11/01 17:37:46 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[52] at collect at utils.scala:181), which has no missing parents
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.2 KB, free 511.0 KB)
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.0 KB, free 519.0 KB)
16/11/01 17:37:46 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:53766 (size: 8.0 KB, free: 511.1 MB)
16/11/01 17:37:46 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:46 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[52] at collect at utils.scala:181)
16/11/01 17:37:46 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
16/11/01 17:37:46 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:37:46 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 12, localhost, partition 1,PROCESS_LOCAL, 2467 bytes)
16/11/01 17:37:46 INFO Executor: Running task 0.0 in stage 10.0 (TID 11)
16/11/01 17:37:46 INFO Executor: Running task 1.0 in stage 10.0 (TID 12)
16/11/01 17:37:46 INFO BlockManager: Found block rdd_42_1 locally
16/11/01 17:37:46 INFO BlockManager: Found block rdd_42_0 locally
16/11/01 17:37:46 INFO Executor: Finished task 1.0 in stage 10.0 (TID 12). 2679 bytes result sent to driver
16/11/01 17:37:46 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 12) in 17 ms on localhost (1/2)
16/11/01 17:37:46 INFO Executor: Finished task 0.0 in stage 10.0 (TID 11). 2679 bytes result sent to driver
16/11/01 17:37:46 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 23 ms on localhost (2/2)
16/11/01 17:37:46 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
16/11/01 17:37:46 INFO DAGScheduler: ShuffleMapStage 10 (collect at utils.scala:181) finished in 0,023 s
16/11/01 17:37:46 INFO DAGScheduler: looking for newly runnable stages
16/11/01 17:37:46 INFO DAGScheduler: running: Set()
16/11/01 17:37:46 INFO DAGScheduler: waiting: Set(ResultStage 11)
16/11/01 17:37:46 INFO DAGScheduler: failed: Set()
16/11/01 17:37:46 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[55] at collect at utils.scala:181), which has no missing parents
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.4 KB, free 528.4 KB)
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.6 KB, free 533.1 KB)
16/11/01 17:37:46 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:53766 (size: 4.6 KB, free: 511.1 MB)
16/11/01 17:37:46 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[55] at collect at utils.scala:181)
16/11/01 17:37:46 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
16/11/01 17:37:46 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 13, localhost, partition 0,NODE_LOCAL, 2290 bytes)
16/11/01 17:37:46 INFO Executor: Running task 0.0 in stage 11.0 (TID 13)
16/11/01 17:37:46 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
16/11/01 17:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
16/11/01 17:37:46 INFO Executor: Finished task 0.0 in stage 11.0 (TID 13). 1830 bytes result sent to driver
16/11/01 17:37:46 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 13) in 13 ms on localhost (1/1)
16/11/01 17:37:46 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
16/11/01 17:37:46 INFO DAGScheduler: ResultStage 11 (collect at utils.scala:181) finished in 0,014 s
16/11/01 17:37:46 INFO DAGScheduler: Job 8 finished: collect at utils.scala:181, took 0,064284 s
16/11/01 17:37:46 INFO ParseDriver: Parsing command: SELECT *
FROM `travel_tags` AS `zzz1`
WHERE (0 = 1)
16/11/01 17:37:46 INFO ParseDriver: Parse Completed
16/11/01 17:37:46 INFO ParseDriver: Parsing command: SELECT *
FROM `travel_tags`
LIMIT 10
16/11/01 17:37:46 INFO ParseDriver: Parse Completed
16/11/01 17:37:46 INFO SparkContext: Starting job: collect at utils.scala:181
16/11/01 17:37:46 INFO DAGScheduler: Got job 9 (collect at utils.scala:181) with 1 output partitions
16/11/01 17:37:46 INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:181)
16/11/01 17:37:46 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:37:46 INFO DAGScheduler: Missing parents: List()
16/11/01 17:37:46 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[58] at collect at utils.scala:181), which has no missing parents
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.9 KB, free 545.0 KB)
16/11/01 17:37:46 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.2 KB, free 551.2 KB)
16/11/01 17:37:46 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:53766 (size: 6.2 KB, free: 511.1 MB)
16/11/01 17:37:46 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[58] at collect at utils.scala:181)
16/11/01 17:37:46 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
16/11/01 17:37:46 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 14, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/11/01 17:37:46 INFO Executor: Running task 0.0 in stage 12.0 (TID 14)
16/11/01 17:37:46 INFO BlockManager: Found block rdd_42_0 locally
16/11/01 17:37:46 INFO GenerateColumnAccessor: Code generated in 21.417316 ms
16/11/01 17:37:46 INFO GenerateSafeProjection: Code generated in 5.982639 ms
16/11/01 17:37:46 INFO Executor: Finished task 0.0 in stage 12.0 (TID 14). 3033 bytes result sent to driver
16/11/01 17:37:46 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 14) in 52 ms on localhost (1/1)
16/11/01 17:37:46 INFO DAGScheduler: ResultStage 12 (collect at utils.scala:181) finished in 0,054 s
16/11/01 17:37:46 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
16/11/01 17:37:46 INFO DAGScheduler: Job 9 finished: collect at utils.scala:181, took 0,065424 s
16/11/01 17:37:46 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
16/11/01 17:37:46 INFO audit: ugi=aissaelouafi	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
16/11/01 17:37:52 INFO ParseDriver: Parsing command: SELECT * FROM travel_tags LIMIT 1000
16/11/01 17:37:52 INFO ParseDriver: Parse Completed
16/11/01 17:37:52 INFO SparkContext: Starting job: collect at utils.scala:181
16/11/01 17:37:52 INFO DAGScheduler: Got job 10 (collect at utils.scala:181) with 1 output partitions
16/11/01 17:37:52 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:181)
16/11/01 17:37:52 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:37:52 INFO DAGScheduler: Missing parents: List()
16/11/01 17:37:52 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[62] at collect at utils.scala:181), which has no missing parents
16/11/01 17:37:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.9 KB, free 563.0 KB)
16/11/01 17:37:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.2 KB, free 569.2 KB)
16/11/01 17:37:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:53766 (size: 6.2 KB, free: 511.0 MB)
16/11/01 17:37:52 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[62] at collect at utils.scala:181)
16/11/01 17:37:52 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
16/11/01 17:37:52 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 15, localhost, partition 0,PROCESS_LOCAL, 2478 bytes)
16/11/01 17:37:52 INFO Executor: Running task 0.0 in stage 13.0 (TID 15)
16/11/01 17:37:52 INFO BlockManager: Found block rdd_42_0 locally
16/11/01 17:37:52 INFO Executor: Finished task 0.0 in stage 13.0 (TID 15). 3033 bytes result sent to driver
16/11/01 17:37:52 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 15) in 11 ms on localhost (1/1)
16/11/01 17:37:52 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:181) finished in 0,012 s
16/11/01 17:37:52 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
16/11/01 17:37:52 INFO DAGScheduler: Job 10 finished: collect at utils.scala:181, took 0,021853 s
16/11/01 17:37:52 INFO SparkContext: Starting job: collect at utils.scala:181
16/11/01 17:37:52 INFO DAGScheduler: Got job 11 (collect at utils.scala:181) with 1 output partitions
16/11/01 17:37:52 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:181)
16/11/01 17:37:52 INFO DAGScheduler: Parents of final stage: List()
16/11/01 17:37:52 INFO DAGScheduler: Missing parents: List()
16/11/01 17:37:52 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[62] at collect at utils.scala:181), which has no missing parents
16/11/01 17:37:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 11.9 KB, free 581.1 KB)
16/11/01 17:37:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.2 KB, free 587.3 KB)
16/11/01 17:37:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:53766 (size: 6.2 KB, free: 511.0 MB)
16/11/01 17:37:52 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
16/11/01 17:37:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[62] at collect at utils.scala:181)
16/11/01 17:37:52 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
16/11/01 17:37:52 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, localhost, partition 1,PROCESS_LOCAL, 2478 bytes)
16/11/01 17:37:52 INFO Executor: Running task 0.0 in stage 14.0 (TID 16)
16/11/01 17:37:52 INFO BlockManager: Found block rdd_42_1 locally
16/11/01 17:37:52 INFO Executor: Finished task 0.0 in stage 14.0 (TID 16). 3030 bytes result sent to driver
16/11/01 17:37:52 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 9 ms on localhost (1/1)
16/11/01 17:37:52 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:181) finished in 0,010 s
16/11/01 17:37:52 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
16/11/01 17:37:52 INFO DAGScheduler: Job 11 finished: collect at utils.scala:181, took 0,028894 s
